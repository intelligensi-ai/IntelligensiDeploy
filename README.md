IntelligensiDeploy
One-Button Cloud Deployments for Intelligensi.ai

IntelligensiDeploy is the unified deployment engine for the entire Intelligensi.ai ecosystem.
It enables one-button, fully automated deployments across GPU and non-GPU infrastructure â€” including image generation servers, AI inference nodes, Weaviate vectors, video generation workers, and future micro-services.

This repository defines a declarative, repeatable, codified deployment pipeline using:

Terraform â†’ Provision GPU instances (Lambda Cloud)

Docker â†’ Build & ship containerized services

Bash Harness â†’ Orchestrate deploy flows

Environment Profiles â†’ dev, staging, prod

Codex-compatible scripts â†’ Every step machine-editable and automated

Our goal:
Click once â†’ launch the entire AI stack.
Zero manual SSH. Zero drift. Zero guesswork.

ğŸš€ Features (Phase 1 & 2)
âœ… 1. GPU Provisioning (Lambda Cloud via Terraform)

Located in deploy/engines/lambda-gpu/:

Creates GPU instances (A10, A100, etc.)

Injects secure SSH keys

Outputs public IPs into service env files

Fully reproducible via terraform apply / destroy

âœ… 2. Container Deployment Engine

Scripts under scripts/ provide:

Docker build + tag

Remote Docker install (if missing)

Push & run container on GPU node

Automatic restarts & cleanup

dev, staging, prod modes

âœ… 3. Image Generation Server Deployment (Flux/SDXL)

The included scripts allow:

./scripts/provision_lambda_gpu.sh  
./scripts/deploy_image_server.sh dev  


Then hit:
http://GPU_IP:8080/

ğŸ”œ 4. Planned Services

This repository will expand to deploy:

Open-source Weaviate cluster

Video generation nodes

OpenAI-compatible inference servers

Vectorization workers

API Gateway for multi-model routing

ğŸ“¦ Repository Structure
IntelligensiDeploy/
â”‚
â”œâ”€â”€ deploy/
â”‚   â””â”€â”€ engines/
â”‚       â””â”€â”€ lambda-gpu/
â”‚           â”œâ”€â”€ main.tf
â”‚           â”œâ”€â”€ variables.tf
â”‚           â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ provision_lambda_gpu.sh
â”‚   â”œâ”€â”€ destroy_lambda_gpu.sh
â”‚   â”œâ”€â”€ deploy_image_server.sh   (coming next)
â”‚   â””â”€â”€ health_check.sh          (coming next)
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ image-server/
â”‚       â””â”€â”€ service.env          # auto-populated with GPU IP
â”‚
â””â”€â”€ README.md

âš™ï¸ Quick Start (Developer Workflow)
1ï¸âƒ£ Export your Lambda API Key
export LAMBDALABS_API_KEY=YOUR_KEY_HERE

2ï¸âƒ£ Provision a GPU Node
./scripts/provision_lambda_gpu.sh


This will:

Spin up a Lambda GPU instance

Output the IP

Write it into services/image-server/service.env

3ï¸âƒ£ Deploy the Image Generation Server
./scripts/deploy_image_server.sh dev

4ï¸âƒ£ Verify
curl http://$(grep DEV_HOST services/image-server/service.env | cut -d= -f2):8080/

ğŸ”¥ Vision

IntelligensiDeploy will evolve into:

A full GUI â€œDeployment Dashboardâ€

With One-Button deploys for:

Image servers

Weaviate

OpenAI-compatible inference engines

LangGraph-based agents

Worker clusters

All driven by GitHub Actions + Terraform automation

This repo is the foundation of Intelligensi.aiâ€™s cloud-native AI infrastructure.

ğŸ¤ Contributing

This project is developed openly as part of the Intelligensi.ai platform.
Pull requests and improvements to deployment flows, reliability, docs, and automation are welcome.

ğŸ§™ Author

Intelligensi.ai â€” AI Infrastructure for the Next Generation of Content Intelligence